---
title: パワースペクトルの変換に関するメモ
category: ['note']
issues: ['module/vox']
---

かなり久しぶりだけど、うまくは行ってないのでアイディアのメモだけ。

## 今の変換の問題の話 ##

今はパワーを $$ (0, \infty) $$ の範囲に正規化[^1]ための $$ f(x) $$ として、 $$ -\log_e{x} $$ を使っている。これは $$ f(0) = \infty $$ [^2] で $$ f(1) = 0 $$。

ただこれだと、畳み込み層の padding が 0 埋めのせいで、スペクトログラムの縁がすごいパワーが高い感じになってしまう。そのため、$$ f(0) = 0, f(1) = \infty $$ な関数を使いたい[^3]。

これは単純に $$ f(x) = -\log_e(1 - x) $$ とすれば良さそうだ。計算コストも安そうだし（楽観主義）。

さて、そうと決まれば派生した話をしなきゃいけない。学習した値からパワーに戻す方法と、inf が出ないようにする方法だ。

## inf が出ないようにする ##

深層学習では基本的に一箇所でも inf が出てしまうと、芋づる式に全ての値が inf と NaN で埋め尽くされてしまう！　これを回避するために、例えば $$ \frac{1}{x} $$ を $$ \frac{1}{x + \varepsilon} $$ に変えたりする（ただし、$$ \varepsilon $$ は $$ 10^{-8} $$ とかの適当な小さな値）。

個人的な趣味で下界を維持したいから、次のようにする。

$$
f(x) = -\log_e(1 - x(1 - \varepsilon))
$$

## パワーに戻す ##

softplus の値域は $$ (0, \infty) $$ で、是非ともこれをパワー $$ x \in (0, 1) $$ に戻したい。使う式の逆だ。

$$
f^{-1}(x) = 1 - e^{-x}
$$

これでいいだろう。

## 最後に ## 

新しい式は決まった。これで padding のせいで値が変になるなんてことはないだろう（予想の上では）。

今から新しい式を使って学習してみようと思う。

良さそうだったら今度報告するね。バイバイ！

脚注
====

[^1]: どうしてもってわけじゃないけど、出力層の活性化に softplus を使ってみたいんだ。

[^2]: ああ、IEEE Float での話ね！（念のため。）

[^3]: ここではどういうわけか、パワーが既に $$ (0, 1) $$ で正規化されている前提で話している。その方が取り回しがいいと思ってるから、そうするようにしてるんだ。